{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a853a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from collections import Counter\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0680010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9340ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the csv.\n",
    "data_df = pd.read_csv(\"Resources/sample_data.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c211c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using drop() to delete rows before 2019\n",
    "data_df.drop(data_df[data_df['Year'] < 2019].index, inplace = True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = data_df.drop(columns=[\"C_S\"])\n",
    "# getting poor results with only dropping country, state so I will drop other features\n",
    "data_df = data_df.drop(columns=[\"C_S\", \"WA\",\"BA\",\"IA\",\"AA\",\"NA\",\"TOM\",\"NH\",\"H\"])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "scaler.fit(data_df)\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056aa8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scaled data\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=data_df.columns)\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A TEST CELL\n",
    "# I'm not getting any accuracy so I'm going to try removing features and seeing if the accuracy \n",
    "\n",
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "# testing to see if the neural network can predict RUCC\n",
    "# changing Cost to RUCC\n",
    "\n",
    "y = scaled_data[\"Cost\"].values\n",
    "# X = scaled_data[[\"Year\",\"FIPS\",\"month\",\"cases\",\"POPESTIMATE\",\"RUCC\"]].values\n",
    "# Trying just one feature\n",
    "X = scaled_data[\"RUCC\"].values.reshape(-1, 1)\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = scaled_data[\"Cost\"].values\n",
    "X = scaled_data.drop([\"Cost\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler instance\n",
    "X_scaler = skl.preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras Sequential model\n",
    "nn_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_df.axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our first Dense layer, including the input layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"relu\", input_dim=1)) #len(data_df.axes[1])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d61025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer that uses a probability activation function\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a55a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the Sequential model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b92ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
